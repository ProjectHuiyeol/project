{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사전 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFElectraModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "np.set_printoptions(precision=6, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NUMBER = 3\n",
    "NEG_CLASS_NUMBER = 5\n",
    "MAX_LEN = 40\n",
    "MAX_WORD = 20\n",
    "BERT_CKPT = './data_out/'\n",
    "DATA_IN_PATH = '../metadata/'\n",
    "DATA_OUT_PATH = './data_out/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토크나이저, 라벨인코더 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Electra Tokenizer\n",
    "with open(DATA_OUT_PATH+'tokenizer.pickle', 'rb') as handle:\n",
    "    loaded_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 3classes(neg, neut, pos) LabelEncoder\n",
    "with open(DATA_OUT_PATH+'3classEncoder.pickle', 'rb') as handle:\n",
    "    le = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 5classes(angry, dislike, fear, sad, surprise) LabelEncoder\n",
    "with open(DATA_OUT_PATH+'negClassEncoder.pickle', 'rb') as handle:\n",
    "    neg_le = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuctiong for Tokenizing Sentence\n",
    "def electra_tokenizer(sent, MAX_LEN):\n",
    "    encoded_dict = loaded_tokenizer.encode_plus(\n",
    "        text=sent,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "\n",
    "    return input_id, attention_mask, token_type_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFElectraClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = TFElectraModel.from_pretrained(model_name, cache_dir=dir_path, from_pt=True)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.classifier = tf.keras.layers.Dense(num_class, name='classifier', activation='softmax', kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range))\n",
    "\n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "        \n",
    "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        last_hidden_state = outputs[0]\n",
    "        last_hidden_state = self.flatten(last_hidden_state)\n",
    "        last_hidden_state = self.dropout(last_hidden_state, training=training)\n",
    "        logits = self.classifier(last_hidden_state)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'electra.embeddings.position_ids', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing TFElectraModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFElectraModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# ElectraClassifier for classifying 3classes(neg, neut, pos)\n",
    "cls_model = TFElectraClassifier(model_name='monologg/koelectra-base-v3-discriminator', dir_path=os.path.join(BERT_CKPT, 'model'), num_class=CLASS_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'electra.embeddings.position_ids', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing TFElectraModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFElectraModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# ElectraClassifier for classifying 5classes(angry, dislike, fear, sad, surprise)\n",
    "neg_model = TFElectraClassifier(model_name='monologg/koelectra-base-v3-discriminator', dir_path=os.path.join(BERT_CKPT, 'model'), num_class=NEG_CLASS_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Loading Weights\n",
    "a, b, c = electra_tokenizer('안녕하세요', MAX_LEN)\n",
    "cls_model.call((np.array(a).reshape(1,-1), np.array(b).reshape(1,-1), np.array(c).reshape(1,-1)))\n",
    "cls_model.built = True\n",
    "cls_model.load_weights(DATA_OUT_PATH+'tf2_electra_plutchik_hs_6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Loading Weights\n",
    "a, b, c = electra_tokenizer('안녕하세요', MAX_LEN)\n",
    "neg_model.call((np.array(a).reshape(1,-1), np.array(b).reshape(1,-1), np.array(c).reshape(1,-1)))\n",
    "neg_model.built = True\n",
    "neg_model.load_weights(DATA_OUT_PATH+'tf2_electra_plutchik_hs_10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "optimizer = tf.keras.optimizers.Adam(3e-6)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Preprocessing 3classes classification(neg, neut, pos)\n",
    "def preprocessing(x):\n",
    "    temp = x.split('\\n')\n",
    "    temp = ' '.join(temp)\n",
    "    return temp\n",
    "\n",
    "def slicing(x):\n",
    "    x = x.split()\n",
    "    res = []\n",
    "    for i in range(0, len(x)+1, MAX_WORD):\n",
    "        if len(x)-i-MAX_WORD < MAX_WORD//2:\n",
    "            temp = x[i:]\n",
    "            res.append(temp)\n",
    "            break\n",
    "        temp = x[i:i+MAX_WORD]\n",
    "        res.append(temp)\n",
    "    return res\n",
    "\n",
    "def predict(lyrics):\n",
    "    temp = []\n",
    "    lyrics = preprocessing(lyrics)\n",
    "    lyrics = slicing(lyrics)\n",
    "    for lyric in lyrics:\n",
    "        try:\n",
    "            txt = ' '.join(lyric)\n",
    "            temp.append(txt)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    for lyric in temp:\n",
    "        try:\n",
    "            input_id, attention_mask, token_type_id = electra_tokenizer(lyric, MAX_LEN)\n",
    "            input_ids.append(input_id)\n",
    "            attention_masks.append(attention_mask)\n",
    "            token_type_ids.append(token_type_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    prediction = cls_model.predict(input_ids)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def transform(array):\n",
    "    result = le.inverse_transform([np.argmax(array)])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Preprocessing 5classes classification(angry, dislike, fear, sad, surprise)\n",
    "def neg_predict(lyrics):\n",
    "    temp = []\n",
    "    lyrics = preprocessing(lyrics)\n",
    "    lyrics = slicing(lyrics)\n",
    "    for lyric in lyrics:\n",
    "        try:\n",
    "            txt = ' '.join(lyric)\n",
    "            temp.append(txt)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    token_type_ids = []\n",
    "\n",
    "    for lyric in temp:\n",
    "        try:\n",
    "            input_id, attention_mask, token_type_id = electra_tokenizer(lyric, MAX_LEN)\n",
    "            input_ids.append(input_id)\n",
    "            attention_masks.append(attention_mask)\n",
    "            token_type_ids.append(token_type_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    prediction = neg_model.predict(input_ids)\n",
    "\n",
    "    return prediction\n",
    "\n",
    "def neg_transform(array):\n",
    "    result = neg_le.inverse_transform([np.argmax(array)])\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = pd.read_csv(DATA_IN_PATH+'db에넣을노래들.csv')\n",
    "data = lyrics[['SONG_ID', 'SONG_TITLE', 'LYRICS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_ids = lyrics['SONG_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 147ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 124ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "2/2 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "2/2 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 212ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n"
     ]
    }
   ],
   "source": [
    "for id in song_ids:\n",
    "    lyric = data[data['SONG_ID']==id]['LYRICS'].values[0]\n",
    "    prediction = predict(lyric)\n",
    "    score = sum(prediction)/len(prediction)\n",
    "    senti = transform(score)\n",
    "    pred[id] = (score, senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data['score'] = ''\n",
    "data['senti'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "for id in song_ids:\n",
    "    sc, se = pred[id]\n",
    "    data.loc[data[data['SONG_ID']==id].index, 'score'] = str(sc)\n",
    "    data.loc[data[data['SONG_ID']==id].index, 'senti'] = se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_data = data[data['senti']=='neg']\n",
    "pos_data = data[data['senti']=='pos']\n",
    "neg_ids = neg_data['SONG_ID']\n",
    "pos_ids = pos_data['SONG_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pred = dict()\n",
    "neg_pred = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 140ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 148ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "2/2 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 120ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n"
     ]
    }
   ],
   "source": [
    "for id in neg_ids:\n",
    "    lyric = data[data['SONG_ID']==id]['LYRICS'].values[0]\n",
    "    prediction = neg_predict(lyric)\n",
    "    score = sum(prediction)/len(prediction)\n",
    "    senti = neg_transform(score)\n",
    "    neg_pred[id] = (score, senti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "data['final_score'] = ''\n",
    "data['final_senti'] = ''\n",
    "data['total'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "for key in neg_ids:\n",
    "    sc, se = neg_pred[key]\n",
    "    data.loc[data[data['SONG_ID']==key].index, 'final_score'] = str(sc)\n",
    "    data.loc[data[data['SONG_ID']==key].index, 'final_senti'] = se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in neg_ids:\n",
    "    neg_sc = np.fromstring(data.loc[data[data['SONG_ID']==key].index, 'final_score'].tolist()[0][1:-1], dtype=float, sep=' ')\n",
    "    pos_sc = np.fromstring(data.loc[data[data['SONG_ID']==key].index, 'score'].tolist()[0][1:-1], dtype=float, sep=' ')\n",
    "    ratio = np.array(((pos_sc[int(le.transform(['neg'])[0])]+pos_sc[1]*0.5), (pos_sc[int(le.transform(['pos'])[0])]+pos_sc[1]*0.5)))\n",
    "    rated_sc = np.append(np.max(neg_sc)*(ratio[1]/ratio[0]), neg_sc)\n",
    "    total_sc = rated_sc/(sum(rated_sc))\n",
    "    data.loc[data[data['SONG_ID']==key].index, 'total'] = str(total_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 328ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 123ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 133ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 134ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 117ms/step\n",
      "1/1 [==============================] - 0s 135ms/step\n",
      "1/1 [==============================] - 0s 150ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "2/2 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 138ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n"
     ]
    }
   ],
   "source": [
    "for id in pos_ids:\n",
    "    lyric = data[data['SONG_ID']==id]['LYRICS'].values[0]\n",
    "    prediction = neg_predict(lyric)\n",
    "    score = sum(prediction)/len(prediction)\n",
    "    pos_pred[id] = (score, 'happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in pos_ids:\n",
    "    sc, se = pos_pred[key]\n",
    "    data.loc[data[data['SONG_ID']==key].index, 'final_score'] = str(sc)\n",
    "    data.loc[data[data['SONG_ID']==key].index, 'final_senti'] = se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in pos_ids:\n",
    "    neg_sc = np.fromstring(data.loc[data[data['SONG_ID']==key].index, 'final_score'].tolist()[0][1:-1], dtype=float, sep=' ')\n",
    "    pos_sc = np.fromstring(data.loc[data[data['SONG_ID']==key].index, 'score'].tolist()[0][1:-1], dtype=float, sep=' ')\n",
    "    ratio = np.array(((pos_sc[int(le.transform(['neg'])[0])]+pos_sc[1]*0.5), (pos_sc[int(le.transform(['pos'])[0])]+pos_sc[1]*0.5)))\n",
    "    rated_sc = np.append(ratio[1], ratio[0]*neg_sc)\n",
    "    total_sc = rated_sc/(sum(rated_sc))\n",
    "    data.loc[data[data['SONG_ID']==key].index, 'total'] = str(total_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data[data['senti']=='neut'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(DATA_IN_PATH+'testLyrics.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassfier():\n",
    "    def __init__(self,encoder1, encoder2):\n",
    "\n",
    "        self.model1 = TFElectraClassifier(model_name='monologg/koelectra-base-v3-discriminator', dir_path=os.path.join(BERT_CKPT, 'model'), num_class=CLASS_NUMBER)\n",
    "        self.model2 = TFElectraClassifier(model_name='monologg/koelectra-base-v3-discriminator', dir_path=os.path.join(BERT_CKPT, 'model'), num_class=NEG_CLASS_NUMBER)\n",
    "        self.encoder1 = encoder1\n",
    "        self.encoder2 = encoder2\n",
    "\n",
    "    def call(self):\n",
    "        # For Loading Weights\n",
    "        a, b, c = electra_tokenizer('안녕하세요', MAX_LEN)\n",
    "        self.model1.call((np.array(a).reshape(1,-1), np.array(b).reshape(1,-1), np.array(c).reshape(1,-1)))\n",
    "        self.model1.built = True\n",
    "        self.model1.load_weights(DATA_OUT_PATH+'tf2_electra_plutchik_hs_6.h5')\n",
    "        # For Loading Weights\n",
    "        a, b, c = electra_tokenizer('안녕하세요', MAX_LEN)\n",
    "        self.model2.call((np.array(a).reshape(1,-1), np.array(b).reshape(1,-1), np.array(c).reshape(1,-1)))\n",
    "        self.model2.built = True\n",
    "        self.model2.load_weights(DATA_OUT_PATH+'tf2_electra_plutchik_hs_10.h5')\n",
    "\n",
    "    # Function for Preprocessing 3classes classification(neg, neut, pos)\n",
    "    def preprocessing(self, lyric):\n",
    "        temp = lyric.split('\\n')\n",
    "        temp = ' '.join(temp)\n",
    "        return temp\n",
    "\n",
    "    def slicing(self, lyric):\n",
    "        lyric = lyric.split()\n",
    "        res = []\n",
    "        for i in range(0, len(lyric)+1, MAX_WORD):\n",
    "            if len(lyric)-i-MAX_WORD < MAX_WORD//2:\n",
    "                temp = lyric[i:]\n",
    "                res.append(temp)\n",
    "                break\n",
    "            temp = lyric[i:i+MAX_WORD]\n",
    "            res.append(temp)\n",
    "        return res\n",
    "\n",
    "    def predict3classes(self, lyrics):\n",
    "        temp = []\n",
    "        lyrics = self.preprocessing(lyrics)\n",
    "        lyrics = self.slicing(lyrics)\n",
    "        for lyric in lyrics:\n",
    "            try:\n",
    "                txt = ' '.join(lyric)\n",
    "                temp.append(txt)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        token_type_ids = []\n",
    "\n",
    "        for lyric in temp:\n",
    "            try:\n",
    "                input_id, attention_mask, token_type_id = electra_tokenizer(lyric, MAX_LEN)\n",
    "                input_ids.append(input_id)\n",
    "                attention_masks.append(attention_mask)\n",
    "                token_type_ids.append(token_type_id)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "        prediction = self.model1.predict(input_ids, verbose=0)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    # Function for Preprocessing 5classes classification(angry, dislike, fear, sad, surprise)\n",
    "    def predict_neg(self, lyrics):\n",
    "        temp = []\n",
    "        lyrics = self.preprocessing(lyrics)\n",
    "        lyrics = self.slicing(lyrics)\n",
    "        for lyric in lyrics:\n",
    "            try:\n",
    "                txt = ' '.join(lyric)\n",
    "                temp.append(txt)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        token_type_ids = []\n",
    "\n",
    "        for lyric in temp:\n",
    "            try:\n",
    "                input_id, attention_mask, token_type_id = electra_tokenizer(lyric, MAX_LEN)\n",
    "                input_ids.append(input_id)\n",
    "                attention_masks.append(attention_mask)\n",
    "                token_type_ids.append(token_type_id)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "        prediction = self.model2.predict(input_ids, verbose=0)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def transform_neg(self, array):\n",
    "        result = self.encoder2.inverse_transform([np.argmax(array)])\n",
    "        return result\n",
    "\n",
    "    def transform3classes(self, array):\n",
    "        result = self.encoder1.inverse_transform([np.argmax(array)])\n",
    "        return result\n",
    "\n",
    "    def predictAll(self, lyrics):\n",
    "        # 데이터 입력\n",
    "        data = lyrics[['SONG_ID', 'SONG_TITLE', 'LYRICS']]\n",
    "        song_ids = lyrics['SONG_ID']\n",
    "        # 3분류\n",
    "        pred = dict()\n",
    "        for id in tqdm(song_ids):\n",
    "            lyric = data[data['SONG_ID']==id]['LYRICS'].values[0]\n",
    "            prediction = self.predict3classes(lyric)\n",
    "            score = sum(prediction)/len(prediction)\n",
    "            senti = self.transform3classes(score)\n",
    "            pred[id] = (score, senti)\n",
    "        data['score'] = ''\n",
    "        data['senti'] = ''\n",
    "        for id in song_ids:\n",
    "            sc, se = pred[id]\n",
    "            data.loc[data[data['SONG_ID']==id].index, 'score'] = str(sc)\n",
    "            data.loc[data[data['SONG_ID']==id].index, 'senti'] = se\n",
    "        # 5분류\n",
    "        neg_data = data[data['senti']=='neg']\n",
    "        pos_data = data[data['senti']=='pos']\n",
    "        neg_ids = neg_data['SONG_ID']\n",
    "        pos_ids = pos_data['SONG_ID']\n",
    "        pos_pred = dict()\n",
    "        neg_pred = dict()\n",
    "        # 대분류가 neg인 경우\n",
    "        for id in tqdm(neg_ids):\n",
    "            lyric = data[data['SONG_ID']==id]['LYRICS'].values[0]\n",
    "            prediction = self.predict_neg(lyric)\n",
    "            score = sum(prediction)/len(prediction)\n",
    "            senti = self.transform_neg(score)\n",
    "            neg_pred[id] = (score, senti)\n",
    "        data['final_score'] = ''\n",
    "        data['final_senti'] = ''\n",
    "        data['total'] = ''\n",
    "        for key in neg_ids:\n",
    "            sc, se = neg_pred[key]\n",
    "            data.loc[data[data['SONG_ID']==key].index, 'final_score'] = str(sc)\n",
    "            data.loc[data[data['SONG_ID']==key].index, 'final_senti'] = se\n",
    "        # neg인 경우의 감정 지표 계산\n",
    "        for key in neg_ids:\n",
    "            neg_sc = np.fromstring(data.loc[data[data['SONG_ID']==key].index, 'final_score'].tolist()[0][1:-1], dtype=float, sep=' ')\n",
    "            pos_sc = np.fromstring(data.loc[data[data['SONG_ID']==key].index, 'score'].tolist()[0][1:-1], dtype=float, sep=' ')\n",
    "            ratio = np.array(((pos_sc[int(le.transform(['neg'])[0])]+pos_sc[1]*0.5), (pos_sc[int(le.transform(['pos'])[0])]+pos_sc[1]*0.5)))\n",
    "            rated_sc = np.append(np.max(neg_sc)*(ratio[1]/ratio[0]), neg_sc)\n",
    "            total_sc = rated_sc/(sum(rated_sc))\n",
    "            data.loc[data[data['SONG_ID']==key].index, 'total'] = str(total_sc)\n",
    "        # 대분류가 pos인 경우\n",
    "        for id in tqdm(pos_ids):\n",
    "            lyric = data[data['SONG_ID']==id]['LYRICS'].values[0]\n",
    "            prediction = self.predict_neg(lyric)\n",
    "            score = sum(prediction)/len(prediction)\n",
    "            pos_pred[id] = (score, 'happy')\n",
    "        for key in pos_ids:\n",
    "            sc, se = pos_pred[key]\n",
    "            data.loc[data[data['SONG_ID']==key].index, 'final_score'] = str(sc)\n",
    "            data.loc[data[data['SONG_ID']==key].index, 'final_senti'] = se\n",
    "        # pos인 경우의 감정 지표 계산\n",
    "        for key in pos_ids:\n",
    "            neg_sc = np.fromstring(data.loc[data[data['SONG_ID']==key].index, 'final_score'].tolist()[0][1:-1], dtype=float, sep=' ')\n",
    "            pos_sc = np.fromstring(data.loc[data[data['SONG_ID']==key].index, 'score'].tolist()[0][1:-1], dtype=float, sep=' ')\n",
    "            ratio = np.array(((pos_sc[int(le.transform(['neg'])[0])]+pos_sc[1]*0.5), (pos_sc[int(le.transform(['pos'])[0])]+pos_sc[1]*0.5)))\n",
    "            rated_sc = np.append(ratio[1], ratio[0]*neg_sc)\n",
    "            total_sc = rated_sc/(sum(rated_sc))\n",
    "            data.loc[data[data['SONG_ID']==key].index, 'total'] = str(total_sc)\n",
    "        # 중립 제거\n",
    "        data = data.drop(data[data['senti']=='neut'].index)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'electra.embeddings.position_ids', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing TFElectraModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFElectraModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'electra.embeddings.position_ids', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing TFElectraModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFElectraModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = EmotionClassfier(le, neg_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(DATA_IN_PATH+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.56it/s]\n",
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\ipykernel_launcher.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\ipykernel_launcher.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n",
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\pandas\\core\\indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n",
      "100%|██████████| 10/10 [00:03<00:00,  2.52it/s]\n",
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\ipykernel_launcher.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\ipykernel_launcher.py:138: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\project_huiyeol\\nlp\\project\\venv\\lib\\site-packages\\ipykernel_launcher.py:139: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 10/10 [00:01<00:00,  8.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SONG_ID</th>\n",
       "      <th>SONG_TITLE</th>\n",
       "      <th>LYRICS</th>\n",
       "      <th>score</th>\n",
       "      <th>senti</th>\n",
       "      <th>final_score</th>\n",
       "      <th>final_senti</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9270</td>\n",
       "      <td>매직 카펫 라이드</td>\n",
       "      <td>이렇게 멋진 파란 하늘 위로 \\n날으는 마법 융단을 타고 \\n이렇게 멋진 푸른 세상...</td>\n",
       "      <td>[0.121995 0.293892 0.584113]</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.148379 0.245044 0.067424 0.509761 0.029392]</td>\n",
       "      <td>happy</td>\n",
       "      <td>[0.731059 0.039905 0.065902 0.018133 0.137096 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19807</td>\n",
       "      <td>벌써 일년</td>\n",
       "      <td>처음이라 그래 며칠뒤엔 \\n괜찮아져 \\n그 생각만으로 벌써 일년이 \\n너와 만든 기...</td>\n",
       "      <td>[0.319238 0.256552 0.424209]</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.167217 0.091865 0.019813 0.712458 0.008647]</td>\n",
       "      <td>happy</td>\n",
       "      <td>[0.552486 0.074832 0.041111 0.008867 0.318835 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32616</td>\n",
       "      <td>3!4!</td>\n",
       "      <td>여기 숨쉬는 이 시간은\\n나를 어데로 데려갈까\\n많은 기쁨과 한숨들이\\n뒤섞인 이곳...</td>\n",
       "      <td>[0.069013 0.425081 0.505906]</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.054421 0.044274 0.028501 0.866249 0.006555]</td>\n",
       "      <td>happy</td>\n",
       "      <td>[0.718446 0.015322 0.012465 0.008025 0.243895 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33133</td>\n",
       "      <td>DOC와 춤을</td>\n",
       "      <td>젓가락질 잘해야만 밥을 먹나요 \\n잘못해도 서툴러도 밥 잘 먹어요 \\n그러나 주위사...</td>\n",
       "      <td>[0.38072  0.380664 0.238616]</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.138274 0.582804 0.094649 0.175393 0.00888 ]</td>\n",
       "      <td>dislike</td>\n",
       "      <td>[0.304481 0.096172 0.405351 0.06583  0.121989 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70696</td>\n",
       "      <td>취중진담(醉中眞談)</td>\n",
       "      <td>그래 난 취했는지도 몰라 실수인지도 몰라 \\n아침이면 까마득히 생각이 안나 \\n불안...</td>\n",
       "      <td>[0.519385 0.197966 0.282649]</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.062054 0.205801 0.140273 0.587172 0.0047  ]</td>\n",
       "      <td>sad</td>\n",
       "      <td>[0.26599  0.045548 0.15106  0.102962 0.43099  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>81013</td>\n",
       "      <td>커플</td>\n",
       "      <td>예전보다 지금 니가\\n더욱 괜찮을거야 \\n허전했던 나의 빈 곳을\\n이젠 채워 줬으니...</td>\n",
       "      <td>[0.141428 0.213228 0.645344]</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.104817 0.078981 0.04542  0.742258 0.028525]</td>\n",
       "      <td>happy</td>\n",
       "      <td>[0.751958 0.025999 0.019591 0.011266 0.184111 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>85257</td>\n",
       "      <td>바꿔</td>\n",
       "      <td>모두 제정신이 아니야 다들  \\n미쳐가고만 있어  \\n어느 누굴 믿어 어찌믿어  \\...</td>\n",
       "      <td>[0.450147 0.353492 0.196361]</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.22911  0.256968 0.035553 0.454508 0.023862]</td>\n",
       "      <td>sad</td>\n",
       "      <td>[0.212914 0.180329 0.202256 0.027983 0.357737 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>85258</td>\n",
       "      <td>와</td>\n",
       "      <td>사실이 아니길 믿고 싶었어 \\n널 놓치기 싫었어 \\n혹시나 우리의 사랑이 잘못 돼\\...</td>\n",
       "      <td>[0.622023 0.177687 0.20029 ]</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.075827 0.162271 0.031417 0.728991 0.001494]</td>\n",
       "      <td>sad</td>\n",
       "      <td>[0.228696 0.058486 0.12516  0.024232 0.562274 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>87048</td>\n",
       "      <td>땡벌</td>\n",
       "      <td>아 당신은 못믿을 사람\\n아 당신은 철없는 사람\\n아무리 달래봐도\\n어쩔순 없지만\\...</td>\n",
       "      <td>[0.78938  0.114164 0.096456]</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.093934 0.392661 0.029438 0.482071 0.001896]</td>\n",
       "      <td>sad</td>\n",
       "      <td>[0.080411 0.086381 0.361087 0.027071 0.443307 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>105014</td>\n",
       "      <td>제주도 푸른밤</td>\n",
       "      <td>떠나요 둘이서 모든 것 훌훌 버리고 \\n제주도 푸른밤 그 별 아래 \\n\\n이제는 더...</td>\n",
       "      <td>[0.178256 0.241083 0.580661]</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.067633 0.283215 0.029714 0.617043 0.002395]</td>\n",
       "      <td>happy</td>\n",
       "      <td>[0.701202 0.020209 0.084624 0.008878 0.184371 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>105187</td>\n",
       "      <td>J에게</td>\n",
       "      <td>J 스치는 바람에\\nJ 그대 모습 보이면\\n난 오늘도 조용히\\n그댈 그리워 하네\\n...</td>\n",
       "      <td>[0.196689 0.206673 0.596638]</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.007424 0.008976 0.054459 0.923036 0.006104]</td>\n",
       "      <td>happy</td>\n",
       "      <td>[0.699975 0.002227 0.002693 0.016339 0.276934 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>111209</td>\n",
       "      <td>다시 사랑한다 말할까</td>\n",
       "      <td>마치 어제 만난 것처럼 \\n잘있었냔 인사가 무색할 만큼 \\n괜한 우려였는지 \\n서먹...</td>\n",
       "      <td>[0.498745 0.207165 0.29409 ]</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.048266 0.100114 0.083173 0.750858 0.017588]</td>\n",
       "      <td>sad</td>\n",
       "      <td>[0.331433 0.032269 0.066933 0.055607 0.501999 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>420534</td>\n",
       "      <td>체념</td>\n",
       "      <td>행복했어\\n너와의 시간들\\n아마도\\n너는 힘들었겠지\\n너의 마음을\\n몰랐던 건 아니...</td>\n",
       "      <td>[0.692555 0.192985 0.11446 ]</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.203763 0.153859 0.037356 0.60309  0.001932]</td>\n",
       "      <td>sad</td>\n",
       "      <td>[0.138849 0.175471 0.132496 0.032169 0.519352 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>437205</td>\n",
       "      <td>애정표현</td>\n",
       "      <td>너무나 오래참았어 \\n가슴만 설레 눈감고 \\n난 지금 꿈을 꾸었어 \\n너무도 아름다...</td>\n",
       "      <td>[0.122449 0.228809 0.648742]</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.041703 0.050297 0.090716 0.793448 0.023836]</td>\n",
       "      <td>happy</td>\n",
       "      <td>[0.763146 0.009878 0.011913 0.021486 0.187931 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>450190</td>\n",
       "      <td>우연</td>\n",
       "      <td>우연이라 하기엔 너무나 \\n심각했지 \\n우린 서로가 서로를 모른 \\n척을 해야만했어...</td>\n",
       "      <td>[0.559501 0.242798 0.197701]</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.026936 0.043854 0.156046 0.67226  0.100904]</td>\n",
       "      <td>sad</td>\n",
       "      <td>[0.239573 0.020483 0.033348 0.118662 0.511205 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>454476</td>\n",
       "      <td>하늘을 달리다</td>\n",
       "      <td>두근거렸지 \\n누군가 나의 뒤를 쫓고있었고 \\n검은 절벽 끝 \\n더 이상 발 디딜 ...</td>\n",
       "      <td>[0.227846 0.259639 0.512514]</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.112912 0.091747 0.151267 0.635776 0.008298]</td>\n",
       "      <td>happy</td>\n",
       "      <td>[0.642334 0.040385 0.032815 0.054103 0.227395 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>460396</td>\n",
       "      <td>소주 한 잔</td>\n",
       "      <td>술이 한 잔 생각나는 밤 같이\\n있는 것 같아요 \\n그 좋았던 시절들 이젠 모두 \\...</td>\n",
       "      <td>[0.587445 0.155883 0.256672]</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.046527 0.171326 0.070133 0.689258 0.022756]</td>\n",
       "      <td>sad</td>\n",
       "      <td>[0.257399 0.034551 0.127227 0.052081 0.511844 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>503339</td>\n",
       "      <td>파도</td>\n",
       "      <td>눈이 부시게 아름답던 바다  \\n나의 눈속엔 \\n그 보다 더 고운 너였어 \\n하얀 ...</td>\n",
       "      <td>[0.230584 0.225529 0.543886]</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.032543 0.061068 0.022407 0.862413 0.021569]</td>\n",
       "      <td>happy</td>\n",
       "      <td>[0.656651 0.011174 0.020968 0.007693 0.296109 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>504784</td>\n",
       "      <td>바다의 왕자</td>\n",
       "      <td>다시 돌아온 바닷가\\n왠지 그녀도 왔을까\\n여기저기 둘러보아도\\n부서지는 파도 소리...</td>\n",
       "      <td>[0.377964 0.186177 0.435859]</td>\n",
       "      <td>pos</td>\n",
       "      <td>[0.092365 0.242203 0.095326 0.467671 0.102435]</td>\n",
       "      <td>happy</td>\n",
       "      <td>[0.528948 0.043509 0.11409  0.044904 0.220298 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>505376</td>\n",
       "      <td>이별택시</td>\n",
       "      <td>건너편에 니가 서두르게 \\n택시를 잡고있어 \\n익숙한 니 동네 외치고 있는 너 \\n...</td>\n",
       "      <td>[0.747751 0.212801 0.039448]</td>\n",
       "      <td>neg</td>\n",
       "      <td>[0.125948 0.45484  0.211429 0.201271 0.006512]</td>\n",
       "      <td>dislike</td>\n",
       "      <td>[0.072068 0.116871 0.422061 0.196192 0.186766 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SONG_ID   SONG_TITLE                                             LYRICS  \\\n",
       "0      9270    매직 카펫 라이드  이렇게 멋진 파란 하늘 위로 \\n날으는 마법 융단을 타고 \\n이렇게 멋진 푸른 세상...   \n",
       "1     19807        벌써 일년  처음이라 그래 며칠뒤엔 \\n괜찮아져 \\n그 생각만으로 벌써 일년이 \\n너와 만든 기...   \n",
       "2     32616         3!4!  여기 숨쉬는 이 시간은\\n나를 어데로 데려갈까\\n많은 기쁨과 한숨들이\\n뒤섞인 이곳...   \n",
       "3     33133      DOC와 춤을  젓가락질 잘해야만 밥을 먹나요 \\n잘못해도 서툴러도 밥 잘 먹어요 \\n그러나 주위사...   \n",
       "4     70696   취중진담(醉中眞談)  그래 난 취했는지도 몰라 실수인지도 몰라 \\n아침이면 까마득히 생각이 안나 \\n불안...   \n",
       "5     81013           커플  예전보다 지금 니가\\n더욱 괜찮을거야 \\n허전했던 나의 빈 곳을\\n이젠 채워 줬으니...   \n",
       "6     85257           바꿔  모두 제정신이 아니야 다들  \\n미쳐가고만 있어  \\n어느 누굴 믿어 어찌믿어  \\...   \n",
       "7     85258            와  사실이 아니길 믿고 싶었어 \\n널 놓치기 싫었어 \\n혹시나 우리의 사랑이 잘못 돼\\...   \n",
       "8     87048           땡벌  아 당신은 못믿을 사람\\n아 당신은 철없는 사람\\n아무리 달래봐도\\n어쩔순 없지만\\...   \n",
       "9    105014      제주도 푸른밤  떠나요 둘이서 모든 것 훌훌 버리고 \\n제주도 푸른밤 그 별 아래 \\n\\n이제는 더...   \n",
       "10   105187          J에게  J 스치는 바람에\\nJ 그대 모습 보이면\\n난 오늘도 조용히\\n그댈 그리워 하네\\n...   \n",
       "11   111209  다시 사랑한다 말할까  마치 어제 만난 것처럼 \\n잘있었냔 인사가 무색할 만큼 \\n괜한 우려였는지 \\n서먹...   \n",
       "12   420534           체념  행복했어\\n너와의 시간들\\n아마도\\n너는 힘들었겠지\\n너의 마음을\\n몰랐던 건 아니...   \n",
       "13   437205         애정표현  너무나 오래참았어 \\n가슴만 설레 눈감고 \\n난 지금 꿈을 꾸었어 \\n너무도 아름다...   \n",
       "14   450190           우연  우연이라 하기엔 너무나 \\n심각했지 \\n우린 서로가 서로를 모른 \\n척을 해야만했어...   \n",
       "15   454476      하늘을 달리다  두근거렸지 \\n누군가 나의 뒤를 쫓고있었고 \\n검은 절벽 끝 \\n더 이상 발 디딜 ...   \n",
       "16   460396       소주 한 잔  술이 한 잔 생각나는 밤 같이\\n있는 것 같아요 \\n그 좋았던 시절들 이젠 모두 \\...   \n",
       "17   503339           파도  눈이 부시게 아름답던 바다  \\n나의 눈속엔 \\n그 보다 더 고운 너였어 \\n하얀 ...   \n",
       "18   504784       바다의 왕자  다시 돌아온 바닷가\\n왠지 그녀도 왔을까\\n여기저기 둘러보아도\\n부서지는 파도 소리...   \n",
       "19   505376         이별택시  건너편에 니가 서두르게 \\n택시를 잡고있어 \\n익숙한 니 동네 외치고 있는 너 \\n...   \n",
       "\n",
       "                           score senti  \\\n",
       "0   [0.121995 0.293892 0.584113]   pos   \n",
       "1   [0.319238 0.256552 0.424209]   pos   \n",
       "2   [0.069013 0.425081 0.505906]   pos   \n",
       "3   [0.38072  0.380664 0.238616]   neg   \n",
       "4   [0.519385 0.197966 0.282649]   neg   \n",
       "5   [0.141428 0.213228 0.645344]   pos   \n",
       "6   [0.450147 0.353492 0.196361]   neg   \n",
       "7   [0.622023 0.177687 0.20029 ]   neg   \n",
       "8   [0.78938  0.114164 0.096456]   neg   \n",
       "9   [0.178256 0.241083 0.580661]   pos   \n",
       "10  [0.196689 0.206673 0.596638]   pos   \n",
       "11  [0.498745 0.207165 0.29409 ]   neg   \n",
       "12  [0.692555 0.192985 0.11446 ]   neg   \n",
       "13  [0.122449 0.228809 0.648742]   pos   \n",
       "14  [0.559501 0.242798 0.197701]   neg   \n",
       "15  [0.227846 0.259639 0.512514]   pos   \n",
       "16  [0.587445 0.155883 0.256672]   neg   \n",
       "17  [0.230584 0.225529 0.543886]   pos   \n",
       "18  [0.377964 0.186177 0.435859]   pos   \n",
       "19  [0.747751 0.212801 0.039448]   neg   \n",
       "\n",
       "                                       final_score final_senti  \\\n",
       "0   [0.148379 0.245044 0.067424 0.509761 0.029392]       happy   \n",
       "1   [0.167217 0.091865 0.019813 0.712458 0.008647]       happy   \n",
       "2   [0.054421 0.044274 0.028501 0.866249 0.006555]       happy   \n",
       "3   [0.138274 0.582804 0.094649 0.175393 0.00888 ]     dislike   \n",
       "4   [0.062054 0.205801 0.140273 0.587172 0.0047  ]         sad   \n",
       "5   [0.104817 0.078981 0.04542  0.742258 0.028525]       happy   \n",
       "6   [0.22911  0.256968 0.035553 0.454508 0.023862]         sad   \n",
       "7   [0.075827 0.162271 0.031417 0.728991 0.001494]         sad   \n",
       "8   [0.093934 0.392661 0.029438 0.482071 0.001896]         sad   \n",
       "9   [0.067633 0.283215 0.029714 0.617043 0.002395]       happy   \n",
       "10  [0.007424 0.008976 0.054459 0.923036 0.006104]       happy   \n",
       "11  [0.048266 0.100114 0.083173 0.750858 0.017588]         sad   \n",
       "12  [0.203763 0.153859 0.037356 0.60309  0.001932]         sad   \n",
       "13  [0.041703 0.050297 0.090716 0.793448 0.023836]       happy   \n",
       "14  [0.026936 0.043854 0.156046 0.67226  0.100904]         sad   \n",
       "15  [0.112912 0.091747 0.151267 0.635776 0.008298]       happy   \n",
       "16  [0.046527 0.171326 0.070133 0.689258 0.022756]         sad   \n",
       "17  [0.032543 0.061068 0.022407 0.862413 0.021569]       happy   \n",
       "18  [0.092365 0.242203 0.095326 0.467671 0.102435]       happy   \n",
       "19  [0.125948 0.45484  0.211429 0.201271 0.006512]     dislike   \n",
       "\n",
       "                                                total  \n",
       "0   [0.731059 0.039905 0.065902 0.018133 0.137096 ...  \n",
       "1   [0.552486 0.074832 0.041111 0.008867 0.318835 ...  \n",
       "2   [0.718446 0.015322 0.012465 0.008025 0.243895 ...  \n",
       "3   [0.304481 0.096172 0.405351 0.06583  0.121989 ...  \n",
       "4   [0.26599  0.045548 0.15106  0.102962 0.43099  ...  \n",
       "5   [0.751958 0.025999 0.019591 0.011266 0.184111 ...  \n",
       "6   [0.212914 0.180329 0.202256 0.027983 0.357737 ...  \n",
       "7   [0.228696 0.058486 0.12516  0.024232 0.562274 ...  \n",
       "8   [0.080411 0.086381 0.361087 0.027071 0.443307 ...  \n",
       "9   [0.701202 0.020209 0.084624 0.008878 0.184371 ...  \n",
       "10  [0.699975 0.002227 0.002693 0.016339 0.276934 ...  \n",
       "11  [0.331433 0.032269 0.066933 0.055607 0.501999 ...  \n",
       "12  [0.138849 0.175471 0.132496 0.032169 0.519352 ...  \n",
       "13  [0.763146 0.009878 0.011913 0.021486 0.187931 ...  \n",
       "14  [0.239573 0.020483 0.033348 0.118662 0.511205 ...  \n",
       "15  [0.642334 0.040385 0.032815 0.054103 0.227395 ...  \n",
       "16  [0.257399 0.034551 0.127227 0.052081 0.511844 ...  \n",
       "17  [0.656651 0.011174 0.020968 0.007693 0.296109 ...  \n",
       "18  [0.528948 0.043509 0.11409  0.044904 0.220298 ...  \n",
       "19  [0.072068 0.116871 0.422061 0.196192 0.186766 ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predictAll(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fbd0d2fb8e022489cd09d5e02fdd4dd8f46560ce587c6c59cf966e8913b7e0e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
